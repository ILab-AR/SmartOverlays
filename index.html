<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta charset="UTF-8">
	<title>SmartOverlays</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#a90329">
	<link rel="stylesheet" type="text/css" href="handgestar_stylesheets/normalize.css" media="screen">
	<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" type="text/css" href="handgestar_stylesheets/stylesheet.css" media="screen">
	<link rel="stylesheet" type="text/css" href="handgestar_stylesheets/github-light.css" media="screen">
	<link rel="shortcut icon" href="handgestar_img/favicon.ico" />
	<script type="text/javascript" async
	  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>
	<!-- <script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
		ga('create', 'UA-101653083-3', 'auto');
		ga('send', 'pageview');
	</script> -->
</head>

<body>
	<section class="page-header">
		<h1 class="project-name">SmartOverlays</h1>
		<h2 class="project-tagline">A Visual Saliency Driven Label Placement for Intelligent Human-Computer Interfaces</h2>
<!-- 		<h2 class="project-authors"><a class = ".a_light" href="https://www.linkedin.com/in/neel-rakholia/"><b>Neel Rakholia\(^*\)</b></a> ,<a href="http://home.iiitd.edu.in/~srinidhi13164/"><b>Srinidhi Hegde\(^*\)</b></a>, <a href="https://scholar.google.co.in/citations?user=IJjnjZIAAAAJ&hl=en"><b>Ramya Hebbalaguppe</b></a></h2> -->
		<a href="#video1" class="btn">Demo Video</a>
		<a href="#user-study" class="btn">User Evaluation</a>
		<!-- <a href="#app1" class="btn">EgoGestAR Dataset</a> -->
		<a href="#paper" class="btn">Paper</a>
		<!-- <a href="https://github.com/handgestar/EgoGestAR" target="_blank" class="btn">EgoGestAR Dataset and Codebase</a>
		<a href="https://github.com/handgestar/HandGestAR" target="_blank" class="btn">Videos Dataset for Testing</a> -->
	</section>

	<section class="main-content">
		<h3><a id="welcome-to-handgestar" class="anchor" href="#welcome-to-handgestar" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Abstract</h3>
		<p align="justify"> In augmented reality (AR), the computer generated labels assist in understanding a scene by addition of contextual information. However, naive label placement often results in clutter and occlusion impairing the effectiveness of AR visualization. For label placement, the main objectives to be satisfied are: non occlusion to scene of interest, the proximity of labels to the object, and lastly, temporally coherent labels in a video/live feed. We present a novel method for the placement of labels corresponding to objects of interest in a video/live feed that satisfies the aforementioned objectives. Our proposed framework, SmartOverlays, first identifies the objects and generates corresponding labels using a YOLOv2 [28] in a video frame; at the same time, Saliency Attention Model (SAM) [7] learns eye fixation points that aid in predicting saliency maps; finally, we compute Voronoi partitions of the video frame, choosing the centroids of objects as seed points, to place labels for satisfying the proximity constraints with the object of interest. In addition, our approach incorporates tracking the detected objects in a frame to facilitate temporal coherence that enhances readability of labels. We measure the effectiveness of SmartOverlays framework using three objective metrics: (a) Label Occlusion over Saliency (LOS), (b) temporal jitter metric that we have introduced to quantify jitter in the label placement, (c) computation time for label placement. </p>

		<p><img width="1120" height="330" src="handgestar_img/show_cluttered.png"></p>

		<h3><a id="welcome-to-handgestar" class="anchor" href="#welcome-to-handgestar" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Key Contribution</h3>
		<ol>
			<li> We propose SmartOverlays, a multi-label placement framework on video frames/live feed. This method comprises of a Saliency Attention Model for computing visual saliency, a real-time object detector such as YOLOv2, followed by our novel label placement module. The label placement module utilizes Voronoi partitioning to avoid label/lead-line overlap and adaptive color scheme to facilitate contrastive label color for dynamic backgrounds.
			<li> We introduce object tracking based methods on detected objects to place labels in a temporally coherent fashion. To evaluate the effectiveness of label placement, we introduce a temporal jitter metric.
			<li> We introduce a metric, Label Occlusion over Saliency score (LOS), for measuring the effectiveness of overlay placement as there are no metrics to evaluate label occlusion.
		</ol>

		<h3><a id="the-idea" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Idea</h3>

		<p><img src="handgestar_img/ijcai_pipeline.png"></p><!-- https://handgestar.github.io/ -->
		
		<p align="justify"> We take video frames as input to our pipeline which we pass to object detector and label generator module and SAM for saliency estimation. The object detector and label generator produce bounding box for all the detected objects along with their respective class labels. Thus it also creates object-label correspondences. SAM computes the saliency maps for each of the video frames. In the final module, we compute the overlay position for each label in a frame based on the object-label correspondences, saliency maps and placement objectives.
			<!-- $$\begin{equation}
		<p><img src="handgestar_img/pipeline.png"></p> https://handgestar.github.io/ -->

		<p align="justify"></p>

		<h3><a id="user-study" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>User Evaluation</h3>
		
		<p align="justify"> The objective of user experiments was to find if our algorithm allowed human users to read the the textual labels best and to explore the shortcomings experienced by the subjects. User studies aid in new directions and extensions to our work based on comments. We recruited 21 subjects – 9, 4, 5, 3 subjects in the age group (A-20 − 25), (B - 26 − 30),(C 30 − 35), (D > 35) respectively. We had 13 male and 8 female human evaluators to test the algorithm in a research lab setting. Experiments involved the subjects to view 20 recorded videos,with different video resolutions, from the DIEM dataset which contained labels placed using SmartOverlays. This datasets consisted of varieties of videos from different genres of advertisements, trailers , tv-series with scenes varying from nature to animated cartoons. Also with eye movements, this dataset provides detailed eye-fixation saliency annotations. The users were tasked to rate the following label placement objectives for each video on a Likert rating scale ranging from 1 − 5, 5 being the highest rating.
			<!-- $$\begin{equation}
		<p><img src="handgestar_img/pipeline.png"></p> https://handgestar.github.io/ -->

		<p align="justify"> The label placement objectives and the subjective metrics for user evaluation are listed below:
		<ol>
			<li> <b>Occlusion Avoidance</b>: Does the label cover/overlap with the regions of interest? Here, a rating of 5 means no occlusion with the salient regions of the videos.
			<li> <b>Proximity</b>: Is the label placed close to the corresponding object? A rating of 5 corresponds to the label being very close to the object of interest.
			<li> <b>Temporal Coherence</b>: Are the labels jittery or jumpy? A rating of 5 means seamless transitions of labels in videos.
			<li> <b>Readability</b>: Is the label readable in every frame ? A rating of 5 corresponds to the highest ease with which one can read especially the color of overlay box and text.
			<li> <b>Color Scheme</b>: Does the label font color stand out with respect to the background? Here 5 means contrast between label and background is high and the algorithm used is adaptive in nature.
			<li> <b>Clarity</b>: Do the connectors or the leader lines intersect? Answers could be Yes/No only.
		</ol>

		<p align="justify"> The following graph shows the user ratings for different subjective metrics.

		<p><img src="handgestar_img/chart.png"></p><!-- https://handgestar.github.io/ -->

		<p align="justify"></p>

		<h3><a id="results" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results</h3>

		<p><img src="handgestar_img/results_yolo.png"></p>

		<!-- Published paper -->
		<h3><a id="paper" class="anchor" href="#the-idea" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Paper</h3>
		For more details, please refer to the <a href="https://drive.google.com/open?id=15fEfWE97W-JjtShyMiWapXIdtYFv59Sr">publication</a>. Feel free to cite our work if you find it useful:
		<pre class="line-numbers">
		   <code class="language-latex">
		      
		      @inproceedings{hegde2020smartoverlays,
			  title={Smartoverlays: A visual saliency driven label placement for intelligent human-computer interfaces},
			  author={Hegde, Srinidhi and Maurya, Jitender and Kalkar, Aniruddha and Hebbalaguppe, Ramya},
			  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
			  pages={1121--1130},
			  year={2020}
			}

		   </code>
		</pre>
		
		<!-- Video for project -->
		<h3><a id="video1" class="anchor" href="#video1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Demo Video</h3>
		<p align="justify">Check out the demo video <a href="https://drive.google.com/open?id=15fEfWE97W-JjtShyMiWapXIdtYFv59Sr">here</a>.</p>
		<div class="video-responsive">
		<iframe width="560" height="315" src="https://drive.google.com/open?id=15fEfWE97W-JjtShyMiWapXIdtYFv59Sr" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		</div>

		<!-- <h3><a id="poster" class="anchor" href="#poster" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Poster</h3>

		<p><img src="handgestar_img/poster.png"></p> -->


	</section>

</body>
</html>
